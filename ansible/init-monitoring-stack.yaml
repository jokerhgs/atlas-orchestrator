- name: Initialize Monitoring Stack (Vaulted Shell Version)
  hosts: role_control_plane
  become: yes
  vars_files:
    - group_vars/all/vault.yml
  vars:
    monitoring_namespace: monitoring
    loki_s3_bucket: "{{ lookup('env', 'LOKI_S3_BUCKET') }}"
    tempo_s3_bucket: "{{ lookup('env', 'TEMPO_S3_BUCKET') }}"
    aws_region: "{{ lookup('env', 'AWS_REGION') | default('us-east-1', true) }}"
    grafana_admin_password: "{{ vault_grafana_admin_password }}"
    kubeconfig_path: "/root/.kube/config"

  tasks:
    - name: Ensure Vault Variable is loaded
      fail:
        msg: "The variable 'vault_grafana_admin_password' is not defined. Did you forget to include the vault file or '--ask-vault-pass'?"
      when: vault_grafana_admin_password is not defined

    - name: Add Grafana Helm repository
      shell: |
        helm repo add grafana https://grafana.github.io/helm-charts
        helm repo update
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"

    - name: Create Monitoring Namespace
      shell: "kubectl create namespace '{{ monitoring_namespace }}' --dry-run=client -o yaml | kubectl apply -f -"
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"


    - name: Deploy Loki (Log Aggregation)
      shell: |
        helm upgrade --install loki grafana/loki \
          --namespace '{{ monitoring_namespace }}' \
          --set loki.auth_enabled=false \
          --set deploymentMode=SingleBinary \
          --set loki.commonStorage.type='s3' \
          --set loki.commonStorage.s3.region='{{ aws_region }}' \
          --set loki.commonStorage.s3.bucketnames='{{ loki_s3_bucket }}' \
          --set loki.storage.type='s3' \
          --set loki.storage.bucketNames.chunks='{{ loki_s3_bucket }}' \
          --set loki.storage.bucketNames.ruler='{{ loki_s3_bucket }}' \
          --set loki.storage.bucketNames.admin='{{ loki_s3_bucket }}' \
          --set loki.storage.s3.region='{{ aws_region }}' \
          --set loki.storage.s3.s3ForcePathStyle=false \
          --set loki.schemaConfig.configs[0].from="2024-01-01" \
          --set loki.schemaConfig.configs[0].store="tsdb" \
          --set loki.schemaConfig.configs[0].object_store="s3" \
          --set loki.schemaConfig.configs[0].schema="v13" \
          --set loki.schemaConfig.configs[0].index.prefix="loki_index_" \
          --set loki.schemaConfig.configs[0].index.period="24h" \
          --set lokiCanary.enabled=false \
          --set test.enabled=false \
          --set loki.persistence.enabled=false \
          --set loki.singleBinary.persistence.enabled=false \
          --set loki.resultsCache.enabled=false \
          --set loki.chunksCache.enabled=false \
          --set loki.resources.requests.cpu=100m \
          --set loki.resources.requests.memory=256Mi \
          --set loki.resources.limits.memory=512Mi \
          --set "loki.nodeSelector.capability=monitoring" \
          --set "loki.tolerations[0].key=dedicated" \
          --set "loki.tolerations[0].operator=Equal" \
          --set "loki.tolerations[0].value=monitoring" \
          --set "loki.tolerations[0].effect=NoSchedule"
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"

    - name: Deploy Tempo (Distributed Tracing)
      shell: |
        helm upgrade --install tempo grafana/tempo \
          --namespace '{{ monitoring_namespace }}' \
          --set tempo.storage.trace.backend=s3 \
          --set tempo.storage.trace.s3.bucket='{{ tempo_s3_bucket }}' \
          --set tempo.storage.trace.s3.region='{{ aws_region }}' \
          --set tempo.persistence.enabled=false \
          --set tempo.resources.requests.cpu=50m \
          --set tempo.resources.requests.memory=128Mi \
          --set tempo.resources.limits.memory=256Mi \
          --set "tempo.nodeSelector.capability=monitoring" \
          --set "tempo.tolerations[0].key=dedicated" \
          --set "tempo.tolerations[0].operator=Equal" \
          --set "tempo.tolerations[0].value=monitoring" \
          --set "tempo.tolerations[0].effect=NoSchedule"
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"

    - name: Deploy Grafana (Dashboards)
      shell: |
        helm upgrade --install grafana grafana/grafana \
          --namespace '{{ monitoring_namespace }}' \
          --set adminPassword='{{ grafana_admin_password }}' \
          --set persistence.enabled=false \
          --set service.type=NodePort \
          --set "datasources.datasources\.yaml.apiVersion=1" \
          --set "datasources.datasources\.yaml.datasources[0].name=Loki" \
          --set "datasources.datasources\.yaml.datasources[0].type=loki" \
          --set "datasources.datasources\.yaml.datasources[0].url=http://loki-gateway.{{ monitoring_namespace }}.svc.cluster.local" \
          --set "datasources.datasources\.yaml.datasources[0].access=proxy" \
          --set "datasources.datasources\.yaml.datasources[1].name=Tempo" \
          --set "datasources.datasources\.yaml.datasources[1].type=tempo" \
          --set "datasources.datasources\.yaml.datasources[1].url=http://tempo.{{ monitoring_namespace }}.svc.cluster.local:3100" \
          --set "datasources.datasources\.yaml.datasources[1].access=proxy" \
          --set "nodeSelector.capability=monitoring" \
          --set "tolerations[0].key=dedicated" \
          --set "tolerations[0].operator=Equal" \
          --set "tolerations[0].value=monitoring" \
          --set "tolerations[0].effect=NoSchedule"
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      no_log: true

    - name: Deploy OpenTelemetry Collector
      shell: |
        helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
        helm repo update
        helm upgrade --install otel-collector open-telemetry/opentelemetry-collector \
          --namespace '{{ monitoring_namespace }}' \
          --set image.repository=otel/opentelemetry-collector-contrib \
          --set mode=deployment \
          --set resources.requests.cpu=50m \
          --set resources.requests.memory=64Mi \
          --set config.exporters.otlp.endpoint=tempo.{{ monitoring_namespace }}.svc.cluster.local:4317 \
          --set config.exporters.otlp.tls.insecure=true \
          --set config.exporters.loki.endpoint=http://loki-gateway.{{ monitoring_namespace }}.svc.cluster.local/loki/api/v1/push \
          --set config.service.pipelines.traces.receivers=[otlp] \
          --set config.service.pipelines.traces.processors=[batch] \
          --set config.service.pipelines.traces.exporters=[otlp] \
          --set config.service.pipelines.logs.receivers=[otlp] \
          --set config.service.pipelines.logs.processors=[batch] \
          --set config.service.pipelines.logs.exporters=[loki] \
          --set "nodeSelector.capability=monitoring" \
          --set "tolerations[0].key=dedicated" \
          --set "tolerations[0].operator=Equal" \
          --set "tolerations[0].value=monitoring" \
          --set "tolerations[0].effect=NoSchedule"
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"

    - name: Deploy Promtail (Log Shipper)
      shell: |
        helm upgrade --install promtail grafana/promtail \
          --namespace '{{ monitoring_namespace }}' \
          --set "config.clients[0].url=http://loki-gateway.{{ monitoring_namespace }}.svc.cluster.local/loki/api/v1/push"
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"

    - name: Get Grafana NodePort
      shell: "kubectl get svc grafana -n '{{ monitoring_namespace }}' -o jsonpath='{.spec.ports[0].nodePort}'"
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: grafana_port
      changed_when: false

    - name: Display connection info
      debug:
        msg:
          - "LGTM Stack + OpenTelemetry Deployed!"
          - "Grafana: http://<Any-Node-IP>:{{ grafana_port.stdout }}"
          - "Loki Gateway: http://loki-gateway.{{ monitoring_namespace }}.svc.cluster.local"
          - "Tempo OTLP: http://otel-collector.{{ monitoring_namespace }}.svc.cluster.local:4317"
          - "Username: admin"
          - "Password: {{ grafana_admin_password if grafana_admin_password != 'admin' else '****** (default)' }}"
